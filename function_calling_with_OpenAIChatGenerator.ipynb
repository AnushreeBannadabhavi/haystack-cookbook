{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyWvdrsAJ9LGk5Lfb7YZGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilgeyucel/haystack-cookbook/blob/main/function_calling_with_OpenAIChatGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Calling with OpenAIChatGenerator ðŸ“ž\n",
        "\n",
        "*Notebook by Bilge Yucel ([LI](https://www.linkedin.com/in/bilge-yucel/) & [X (Twitter)](https://twitter.com/bilgeycl))*\n",
        "\n",
        "A guide to understand function calling and how to use OpenAI function calling feature with [Haystack](https://github.com/deepset-ai/haystack).\n",
        "\n",
        "ðŸ“š Useful Sources:\n",
        "* [OpenAIChatGenerator Docs](https://docs.haystack.deepset.ai/v2.0/docs/openaichatgenerator)\n",
        "* [OpenAIChatGenerator API Reference](https://docs.haystack.deepset.ai/v2.0/reference/generator-api#openaichatgenerator)"
      ],
      "metadata": {
        "id": "HJsHqaTksDHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Function Calling with OpenAIChatGenerator ðŸ“ž](#scrollTo=HJsHqaTksDHt)\n",
        "\n",
        ">>[Overview](#scrollTo=PWXXqq_MPn7y)\n",
        "\n",
        ">>[Set up the Development Environment](#scrollTo=K04cnh_IleMV)\n",
        "\n",
        ">>[Learn about the OpenAIChatGenerator](#scrollTo=0_mGdadLrcNr)\n",
        "\n",
        ">>>[Basic Streaming](#scrollTo=cbl0xs0MJ76Z)\n",
        "\n",
        ">>[Function Calling with OpenAIChatGenerator](#scrollTo=HeR1XyNytCBY)\n",
        "\n",
        ">>>[Define a Function](#scrollTo=FWkDXKbeoNqZ)\n",
        "\n",
        ">>>[Create the tools](#scrollTo=zJt-mzb4oHxj)\n",
        "\n",
        ">>>[Run OpenAIChatGenerator with tools](#scrollTo=bkRPp3JKpZgf)\n",
        "\n",
        ">>>[Make a Tool Call](#scrollTo=R1HWsY5_tZ6d)\n",
        "\n",
        ">>[Improve the Example](#scrollTo=_WTrLQr-X1Y2)\n",
        "\n",
        ">>>[Start the Application](#scrollTo=5UkjGYEfwPzS)\n",
        "\n",
        ">>>[Print the summary of the conversation](#scrollTo=XgiXu5bltJJN)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "er85KFKknFS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Here are some use cases of function calling from [OpenAI Docs](https://platform.openai.com/docs/guides/function-calling):\n",
        "* **Create assistants that answer questions by calling external APIs** (e.g. like ChatGPT Plugins)\n",
        "e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')\n",
        "* **Convert natural language into API calls**\n",
        "e.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal API\n",
        "* **Extract structured data from text**\n",
        "e.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)"
      ],
      "metadata": {
        "id": "PWXXqq_MPn7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the Development Environment"
      ],
      "metadata": {
        "id": "K04cnh_IleMV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zNyqNVFaPN1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c49055-1340-4328-b184-43b94abbbf34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: haystack-ai in /usr/local/lib/python3.10/dist-packages (2.0.0b5)\n",
            "Requirement already satisfied: boilerpy3 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.0.7)\n",
            "Requirement already satisfied: haystack-bm25 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.1.3)\n",
            "Requirement already satisfied: lazy-imports in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (0.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.2.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.5.3)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.3.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (6.0.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-bm25->haystack-ai) (1.23.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai) (2.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2023.3.post1)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai) (2.2.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->haystack-ai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->haystack-ai) (2.0.7)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "pip install haystack-ai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY') or getpass(\"OPENAI_API_KEY: \")"
      ],
      "metadata": {
        "id": "WM-sVkYonutA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learn about the OpenAIChatGenerator\n",
        "\n",
        "`OpenAIChatGenerator` is a component that supports the function calling feature of OpenAI.\n",
        "\n",
        "The way to communicate with `OpenAIChatGenerator` is through [`ChatMessage`](https://docs.haystack.deepset.ai/v2.0/docs/data-classes#chatmessage) list. Therefore, create a `ChatMessage` with \"USER\" role using `ChatMessage.from_user()` and send it to OpenAIChatGenerator:"
      ],
      "metadata": {
        "id": "0_mGdadLrcNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "\n",
        "client = OpenAIChatGenerator()\n",
        "response = client.run(\n",
        "    [ChatMessage.from_user(\"What's Natural Language Processing? Be brief.\")]\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZiH3ZUCPZfH",
        "outputId": "f3684af3-2948-4743-f46d-27a1e5a33053"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'replies': [ChatMessage(content='Natural Language Processing (NLP) is a branch of artificial intelligence that deals with the interaction between computers and humans in natural language. It focuses on the understanding, interpretation, and generation of human language to enable machines to process and analyze textual data efficiently.', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0613', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 50, 'prompt_tokens': 16, 'total_tokens': 66}})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Streaming\n",
        "\n",
        "OpenAIChatGenerator supports streaming, provide a `streaming_callback` function and run the client again to see the difference.\n",
        "\n",
        "> We'll have `print_streaming_chunk` util in Haystack w **v2.0.0-beta.6**"
      ],
      "metadata": {
        "id": "cbl0xs0MJ76Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.dataclasses import StreamingChunk\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "\n",
        "def print_streaming_chunk(chunk: StreamingChunk) -> None:\n",
        "    \"\"\"\n",
        "    Default callback function for streaming responses.\n",
        "    Prints the tokens of the first completion to stdout as soon as they are received\n",
        "    \"\"\"\n",
        "    print(chunk.content, flush=True, end=\"\")\n",
        "\n",
        "\n",
        "client = OpenAIChatGenerator(streaming_callback=print_streaming_chunk)\n",
        "response = client.run(\n",
        "    [ChatMessage.from_user(\"What's Natural Language Processing? Be brief.\")]\n",
        ")"
      ],
      "metadata": {
        "id": "D0nEEd5PJ1X2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b845ce78-962c-4d74-e7a3-132f8bcf795c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between humans and computers using natural language. It involves the development of algorithms and methods to enable computers to understand, interpret, and generate human language in a manner that is meaningful and useful."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling with OpenAIChatGenerator\n",
        "\n",
        "We'll try to recreate the [example on OpenAI docs](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models)."
      ],
      "metadata": {
        "id": "HeR1XyNytCBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Function\n",
        "\n",
        "We'll define a `get_current_weather` function that mocks a Weather API call in the response:"
      ],
      "metadata": {
        "id": "FWkDXKbeoNqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_weather(location: str, unit: str = \"celsius\"):\n",
        "  ## Do something\n",
        "  return {\"weather\": \"sunny\", \"temperature\": 21.8, \"unit\": unit}\n",
        "\n",
        "available_functions = {\n",
        "  \"get_current_weather\": get_current_weather\n",
        "}"
      ],
      "metadata": {
        "id": "yV_LM-b7O6KI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the `tools`\n",
        "\n",
        "We'll then add information about this function to our `tools` list by following [OpenAI's tool schema](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools)"
      ],
      "metadata": {
        "id": "zJt-mzb4oHxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"location\", \"unit\"],\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "1C-BzvPQntmd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run OpenAIChatGenerator with tools\n",
        "\n",
        "We'll pass the list of tools in the `run()` method as `generation_kwargs`.\n",
        "\n",
        "Let's define messages and run the generator:"
      ],
      "metadata": {
        "id": "bkRPp3JKpZgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.dataclasses import ChatMessage\n",
        "\n",
        "messages = []\n",
        "messages.append(ChatMessage.from_system(\"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"))\n",
        "messages.append(ChatMessage.from_user(\"What's the weather like in Berlin?\"))\n",
        "\n",
        "client = OpenAIChatGenerator(streaming_callback=print_streaming_chunk)\n",
        "response = client.run(\n",
        "    messages=messages,\n",
        "    generation_kwargs={\"tools\":tools}\n",
        ")\n"
      ],
      "metadata": {
        "id": "OEScMyqctzFN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's a function call! ðŸ“ž The response gives us information about the function name and arguments to use to call that function:"
      ],
      "metadata": {
        "id": "OvPWnfpnQjJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dd2QOOYvIJB",
        "outputId": "bfd50f26-be16-4229-bd83-d8f90038e0dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'replies': [ChatMessage(content='[{\"index\": 0, \"id\": \"call_fFQKCAUba8RRu2BZ4v8IVYPH\", \"function\": {\"arguments\": \"{\\\\n  \\\\\"location\\\\\": \\\\\"Berlin\\\\\",\\\\n  \\\\\"unit\\\\\": \\\\\"celsius\\\\\"\\\\n}\", \"name\": \"get_current_weather\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0613', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, add the message with function information to the message list"
      ],
      "metadata": {
        "id": "Bbxk3gEbvJ_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(response[\"replies\"][0])"
      ],
      "metadata": {
        "id": "VO0kAxGRQmM-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See how we can extract the `function_name` and `function_args` from the message"
      ],
      "metadata": {
        "id": "0mSQij2-OyoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "function_call = json.loads(response[\"replies\"][0].content)[0]\n",
        "function_name = function_call[\"function\"][\"name\"]\n",
        "function_args = json.loads(function_call[\"function\"][\"arguments\"])\n",
        "print(\"function_name:\", function_name)\n",
        "print(\"function_args:\", function_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADK925C6o3uO",
        "outputId": "cbd2d908-1242-4506-82e6-876a9842b362"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "function_name: get_current_weather\n",
            "function_args: {'location': 'Berlin', 'unit': 'celsius'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make a Tool Call\n",
        "\n",
        "Let's locate the corresponding function for `function_name` in our `available_functions` dictionary and use `function_args` when calling it. Once we receive the response from the tool, we'll append it to our `messages` for later sending to OpenAI."
      ],
      "metadata": {
        "id": "R1HWsY5_tZ6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function_to_call = available_functions[function_name]\n",
        "function_response = function_to_call(**function_args)\n",
        "function_message = ChatMessage.from_function(content=json.dumps(function_response), name=function_name)\n",
        "messages.append(function_message)"
      ],
      "metadata": {
        "id": "y2rF5JcytaMX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make the last call to OpenAI with response coming from the function and see how OpenAI uses the provided information"
      ],
      "metadata": {
        "id": "oEB3e_PcZ11n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.run(\n",
        "    messages=messages,\n",
        "    generation_kwargs={\"tools\":tools}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1p6w-uxZ09K",
        "outputId": "97973938-7d43-4216-98b2-23f75c6c99a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current weather in Berlin is sunny with a temperature of 21.8Â°C."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improve the Example\n",
        "\n",
        "Let's add more tool to our example and improve the user experience ðŸ‘‡\n",
        "\n",
        "We'll add one more tool `use_haystack_pipeline` for OpenAI to use when there's a question about countries and capitals:"
      ],
      "metadata": {
        "id": "_WTrLQr-X1Y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"location\", \"unit\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"use_haystack_pipeline\",\n",
        "            \"description\": \"Use for search about countries and capitals\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The query to use in the search. Infer this from the user's message\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"query\"]\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "rfYYSytSVtPm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_weather(location: str, unit: str = \"celsius\"):\n",
        "  return {\"weather\": \"sunny\", \"temperature\": 21.8, \"unit\": unit}\n",
        "\n",
        "def use_haystack_pipeline(query: str):\n",
        "  # It returns a mock response\n",
        "  return {\"documents\": \"Cutopia is the capital of Utopia\", \"query\": query}\n",
        "\n",
        "available_functions = {\n",
        "  \"get_current_weather\": get_current_weather,\n",
        "  \"use_haystack_pipeline\": use_haystack_pipeline,\n",
        "}"
      ],
      "metadata": {
        "id": "VHxeFwAhrNZw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start the Application\n",
        "\n",
        "Have fun having a chat with OpenAI ðŸŽ‰\n",
        "\n",
        "Example queries you can try:\n",
        "* \"***What's the capital of Utopia***\", \"***Is it sunny there?***\": To test the messages are being recorded and sent\n",
        "* \"***What's the weather like in the capital of Utopia?***\": To force two function calls\n",
        "* \"***What's the weather like today?***\": To force OpenAI to ask more clarification"
      ],
      "metadata": {
        "id": "5UkjGYEfwPzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from haystack.dataclasses import ChatMessage, ChatRole\n",
        "\n",
        "messages = []\n",
        "messages.append(ChatMessage.from_system(\"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"))\n",
        "\n",
        "print(messages[-1].content)\n",
        "\n",
        "while True:\n",
        "  # if this is a tool call\n",
        "  if response and response[\"replies\"][0].meta[\"finish_reason\"] == 'tool_calls':\n",
        "    function_calls = json.loads(response[\"replies\"][0].content)\n",
        "    for function_call in function_calls:\n",
        "      function_name = function_call[\"function\"][\"name\"]\n",
        "      function_to_call = available_functions[function_name]\n",
        "      function_args = json.loads(function_call[\"function\"][\"arguments\"])\n",
        "\n",
        "      function_response = function_to_call(**function_args)\n",
        "      function_message = ChatMessage.from_function(content=json.dumps(function_response), name=function_name)\n",
        "      messages.append(function_message)\n",
        "\n",
        "  # Regular Conversation\n",
        "  else:\n",
        "    # If it's not user's first message and there's an assistant message\n",
        "    if not messages[-1].is_from(ChatRole.SYSTEM):\n",
        "      messages.append(ChatMessage.from_assistant(response[\"replies\"][0].content))\n",
        "\n",
        "    user_input = input(\"INFO: Type 'exit' or 'quit' to stop\\n\")\n",
        "    if user_input.lower() == \"exit\" or user_input.lower() == \"quit\":\n",
        "      break\n",
        "    else:\n",
        "      messages.append(ChatMessage.from_user(user_input))\n",
        "\n",
        "  response = client.run(\n",
        "    messages=messages,\n",
        "    generation_kwargs={\"tools\":tools}\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK_JeKZLhXcy",
        "outputId": "8ffaaa59-bae1-4ca1-fd90-eedf768ec94b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\n",
            "INFO: Type 'exit' or 'quit' to stop\n",
            "What's the weather like today?\n",
            "Sure, can you please tell me your current location?INFO: Type 'exit' or 'quit' to stop\n",
            "utopia\n",
            "The weather in Utopia today is sunny with a temperature of 21.8 degrees Celsius.INFO: Type 'exit' or 'quit' to stop\n",
            "exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the summary of the conversation\n",
        "\n",
        "This part can help you understand the message order"
      ],
      "metadata": {
        "id": "XgiXu5bltJJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== SUMMARY ===\")\n",
        "for m in messages:\n",
        "  print(f\"\\n - {m.content}\")"
      ],
      "metadata": {
        "id": "tMom7ESfT_R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c883070-0e9f-42d2-9afc-d0821f0244bb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SUMMARY ===\n",
            "\n",
            " - Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\n",
            "\n",
            " - What's the weather like today?\n",
            "\n",
            " - Sure, can you please tell me your current location?\n",
            "\n",
            " - utopia\n",
            "\n",
            " - {\"weather\": \"sunny\", \"temperature\": 21.8, \"unit\": \"celsius\"}\n",
            "\n",
            " - The weather in Utopia today is sunny with a temperature of 21.8 degrees Celsius.\n"
          ]
        }
      ]
    }
  ]
}