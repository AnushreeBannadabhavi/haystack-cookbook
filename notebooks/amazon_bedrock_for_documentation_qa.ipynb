{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Effortless QA Application Development with Amazon Bedrock and Haystack\n",
        "\n",
        "*Notebook by [Bilge Yucel](https://www.linkedin.com/in/bilge-yucel/)*\n",
        "\n",
        "[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that provides high-performing foundation models from leading AI startups and Amazon through a single API. You can choose from various foundation models to find the one best suited for your use case.\n",
        "\n",
        "In this notebook, we'll go through the process of **creating a generative question answering application** tailored for PDF files using the newly added [Amazon Bedrock integration](https://haystack.deepset.ai/integrations/amazon-bedrock) with [Haystack](https://github.com/deepset-ai/haystack) and [OpenSearch](https://haystack.deepset.ai/integrations/opensearch-document-store) to store our documents efficiently. The demo will illustrate the step-by-step development of a QA application designed specifically for the Bedrock documentation, demonstrating the power of Bedrock in the process 🚀\n",
        "\n",
        "## Setup the Development Environment"
      ],
      "metadata": {
        "id": "YxZBCJn21Ygd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "v5dzhxUV1QwR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX5oCws-etEH"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install opensearch-haystack amazon-bedrock-haystack pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Files\n",
        "\n",
        "For this application, we'll use the user guide of Amazon Bedrock. Amazon Bedrock provides the [PDF form of its guide](https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf). Run the code to download the PDF to `/content/bedrock-documentation.pdf` directory 👇🏼  "
      ],
      "metadata": {
        "id": "WMJaEllC1Wat"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "chi-VAhGeuQn"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.config import Config\n",
        "\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "s3.download_file('core-engineering', 'public/blog-posts/bedrock-documentation.pdf', '/content/bedrock-documentation.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys3-RvVqqWdD"
      },
      "source": [
        "### Initialize an OpenSearch Instance on Colab\n",
        "\n",
        "[OpenSearch](https://opensearch.org/) is a fully open source search and analytics engine and is compatible with the [Amazon OpenSearch Service](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html) that’s helpful if you’d like to deploy, operate, and scale your OpenSearch cluster later on.\n",
        "\n",
        "Let’s install OpenSearch and start an instance on Colab. For other installation options, check out [OpenSearch documentation](https://opensearch.org/docs/latest/install-and-configure/install-opensearch/index/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyWWR3Xye8l_"
      },
      "outputs": [],
      "source": [
        "!wget https://artifacts.opensearch.org/releases/bundle/opensearch/2.11.1/opensearch-2.11.1-linux-x64.tar.gz\n",
        "!tar -xvf opensearch-2.11.1-linux-x64.tar.gz\n",
        "!chown -R daemon:daemon opensearch-2.11.1\n",
        "# disabling security. Be mindful when you want to disable security in production systems\n",
        "!sudo echo 'plugins.security.disabled: true' >> opensearch-2.11.1/config/opensearch.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vaxe75MXkMi2"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "cd opensearch-2.11.1 && sudo -u daemon -- ./bin/opensearch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> OpenSearch needs 30 seconds for a fully started server"
      ],
      "metadata": {
        "id": "YuN1y5WQ1jI9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f9gbVwRU_Y5Q"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "time.sleep(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API Keys\n",
        "\n",
        "To use Amazon Bedrock, you need `aws_access_key_id`, `aws_secret_access_key`, and indicate the `aws_region_name`. Once logged into your account, locate these keys under the IAM user's \"Security Credentials\" section. For detailed guidance, refer to the documentation on [Managing access keys for IAM users](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)."
      ],
      "metadata": {
        "id": "pSBYYgYq1Ij3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZTz7cHwhZ-9"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "aws_access_key_id = getpass(\"aws_access_key_id: \")\n",
        "aws_secret_access_key = getpass(\"aws_secret_access_key: \")\n",
        "aws_region_name = input(\"aws_region_name: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an indexing pipeline\n",
        "\n",
        "Our indexing pipeline will convert the PDF file into a Haystack Document using [PyPDFToDocument](https://docs.haystack.deepset.ai/v2.0/docs/pypdftodocument) and preprocess it by cleaning and splitting it into chunks before storing them in [OpenSearchDocumentStore](https://docs.haystack.deepset.ai/v2.0/docs/opensearch-document-store).\n",
        "\n",
        "Let’s run the pipeline below and index our file to our document store:"
      ],
      "metadata": {
        "id": "oa6aH6fB08d_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SrBctAl5e_Kf"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from haystack import Pipeline\n",
        "from haystack.components.converters import PyPDFToDocument\n",
        "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.document_stores import DuplicatePolicy\n",
        "from opensearch_haystack import OpenSearchDocumentStore\n",
        "\n",
        "## Initialize the OpenSearchDocumentStore\n",
        "document_store = OpenSearchDocumentStore()\n",
        "\n",
        "## Create pipeline components\n",
        "converter = PyPDFToDocument()\n",
        "cleaner = DocumentCleaner()\n",
        "splitter = DocumentSplitter(split_by=\"sentence\", split_length=10, split_overlap=2)\n",
        "writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.SKIP)\n",
        "\n",
        "## Add components to the pipeline\n",
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(\"converter\", converter)\n",
        "indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
        "indexing_pipeline.add_component(\"splitter\", splitter)\n",
        "indexing_pipeline.add_component(\"writer\", writer)\n",
        "\n",
        "## Connect the components to each other\n",
        "indexing_pipeline.connect(\"converter\", \"cleaner\")\n",
        "indexing_pipeline.connect(\"cleaner\", \"splitter\")\n",
        "indexing_pipeline.connect(\"splitter\", \"writer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the pipeline with the files you want to index:"
      ],
      "metadata": {
        "id": "oJLXM8nM02AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexing_pipeline.run({\"converter\": {\"sources\": [Path(\"/content/bedrock-documentation.pdf\")]}})"
      ],
      "metadata": {
        "id": "X7HrON1PFHos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNmHvZLjA4Rv"
      },
      "source": [
        "## Building the query pipeline\n",
        "\n",
        "Let’s create another pipeline to query our application. In this pipeline, we’ll use [OpenSearchBM25Retriever](https://docs.haystack.deepset.ai/v2.0/docs/opensearchbm25retriever) to retrieve relevant information from the OpenSearchDocumentStore and an Amazon Titan model `amazon.titan-text-express-v1` to generate answers with [AmazonBedrockGenerator](https://docs.haystack.deepset.ai/v2.0/docs/amazonbedrockgenerator). You can select and test different models using the dropdown on right.\n",
        "\n",
        "Then, we’ll define a prompt based on our task, in this case, to generate answers based on the given context and connect these three components to each other to finalize the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8Q3JYuyShRnQ"
      },
      "outputs": [],
      "source": [
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.pipeline import Pipeline\n",
        "from amazon_bedrock_haystack.generators.amazon_bedrock import AmazonBedrockGenerator\n",
        "from opensearch_haystack import OpenSearchBM25Retriever\n",
        "\n",
        "## Create pipeline components\n",
        "retriever = OpenSearchBM25Retriever(document_store=document_store, top_k=15)\n",
        "\n",
        "## Initialize the AmazonBedrockGenerator with an Amazon Bedrock model\n",
        "bedrock_model = 'amazon.titan-text-express-v1' # @param [\"amazon.titan-text-express-v1\", \"amazon.titan-text-lite-v1\", \"anthropic.claude-instant-v1\", \"anthropic.claude-v1\", \"anthropic.claude-v2\",\"anthropic.claude-v2:1\", \"meta.llama2-13b-chat-v1\", \"meta.llama2-70b-chat-v1\", \"ai21.j2-mid-v1\", \"ai21.j2-ultra-v1\"]\n",
        "generator = AmazonBedrockGenerator(model_name=bedrock_model,\n",
        "                                   aws_access_key_id=aws_access_key_id,\n",
        "                                   aws_secret_access_key=aws_secret_access_key,\n",
        "                                   aws_region_name=aws_region_name,\n",
        "                                   max_length=500)\n",
        "template = \"\"\"\n",
        "{% for document in documents %}\n",
        "    {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Please answer the question based on the given information from Amazon Bedrock documentation.\n",
        "\n",
        "{{question}}\n",
        "\"\"\"\n",
        "prompt_builder = PromptBuilder(template=template)\n",
        "\n",
        "## Add components to the pipeline\n",
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_component(\"retriever\", retriever)\n",
        "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "rag_pipeline.add_component(\"llm\", generator)\n",
        "\n",
        "## Connect the components to each other\n",
        "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "rag_pipeline.connect(\"prompt_builder\", \"llm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ask your question and learn about the Amazon Bedrock service using Amazon Bedrock models!"
      ],
      "metadata": {
        "id": "5NywqZKo6msf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mDYCSRRtiAy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671da188-ed15-48e7-958f-0e1343faf458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a uniﬁed API. You can choose from a wide range of foundation models to ﬁnd the model that is best suited for your use case. Amazon Bedrock also oﬀers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as ﬁne-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.\n",
            "With Amazon Bedrock's serverless experience, you can get started quickly, privately customize foundation models with your own data, and easily and securely integrate and deploy them into your applications using AWS tools without having to manage any infrastructure.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is Amazon Bedrock?\"\n",
        "response = rag_pipeline.run({\"retriever\": {\"query\": question}, \"prompt_builder\": {\"question\": question}})\n",
        "\n",
        "print(response[\"llm\"][\"replies\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "giSWajzyAcNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ec1dcc6-4a28-4815-f9b4-7c073584c20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To setup Amazon Bedrock, follow these steps:\n",
            "1. Sign up for an AWS account\n",
            "2. Create an administrative user\n",
            "3. Grant programmatic access\n",
            "4. Console access\n",
            "5. Model access\n",
            "6. Set up the Amazon Bedrock API\n"
          ]
        }
      ],
      "source": [
        "question = \"How can I setup Amazon Bedrock?\"\n",
        "response = rag_pipeline.run({\"retriever\": {\"query\": question}, \"prompt_builder\": {\"question\": question}})\n",
        "\n",
        "print(response[\"llm\"][\"replies\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How can I finetune foundation models?\"\n",
        "response = rag_pipeline.run({\"retriever\": {\"query\": question}, \"prompt_builder\": {\"question\": question}})\n",
        "\n",
        "print(response[\"llm\"][\"replies\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROhJ8VL_JdHc",
        "outputId": "76f73ed3-4fb3-4def-b88e-bbfde218cb7e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon Bedrock supports fine-tuning of foundation models through model customization jobs. You can create a model customization job by providing a training dataset that you own. During a model customization job, you can tune the hyperparameters of the model to improve its performance on your specific task or domain. For more information, see Custom models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How should I form my prompts for Amazon Titan models?\"\n",
        "response = rag_pipeline.run({\"retriever\": {\"query\": question}, \"prompt_builder\": {\"question\": question}})\n",
        "\n",
        "print(response[\"llm\"][\"replies\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ohsesYDgsSq",
        "outputId": "2ab326d4-4279-424a-a826-10dfcbe3c00d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When formulating your prompts for Amazon Titan models, it's important to consider the task or instruction you want the models to perform, the context of the task, demonstration examples, and the input text that you want the models to use in their response. Depending on your use case, the availability of data, and the task, your prompt should combine one or more of these components.\n",
            "\n",
            "For example, if you're asking the model to summarize a text, you might include the text itself, the task or instruction (\"Summarize the text\"), demonstration examples (\"Example summaries\"), and the input text (\"The following is text from a restaurant review: \\\"I finally got to check out Alessandro's Brilliant Pizza and it is now one of my favorite restaurants in Seattle. The dining room has a beautiful view over the Puget Sound but it was surprisingly not crowded. I ordered the fried castelvetrano olives, a spicy Neapolitan-style pizza and a gnocchi dish. The olives were absolutely decadent, and the pizza came with a smoked mozzarella, which was delicious. The gnocchi was fresh and wonderful. The waitstaff were attentive, and overall the experience was lovely. I hope to return soon.\\\"\")\n",
            "\n",
            "By providing this information, you can help the model understand the task and generate a more accurate and relevant response.\n",
            "\n",
            "Remember that the success of Amazon Titan models depends on the quality and relevance of your prompts, so it's essential to spend time experimenting and refining your approach to get the best results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How should I form my prompts for Claude models?\"\n",
        "response = rag_pipeline.run({\"retriever\": {\"query\": question}, \"prompt_builder\": {\"question\": question}})\n",
        "\n",
        "print(response[\"llm\"][\"replies\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__NGWZdqh_dJ",
        "outputId": "26c84bf3-9401-438a-bac5-1032a5d9c20e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When using Claude models with Amazon Bedrock, it's important to wrap your prompts in a conversational style to get desired responses. The main content of the prompt should be wrapped like this: \\n\\nHuman: {{Main Content}}\\n\\nAssistant:. For Claude models, prompts sent via the API must contain \\n\\nHuman: and \\n\\nAssistant:.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}